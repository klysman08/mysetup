# DS ENV

```bash
chmod +x ds-init.sh

# Criar projeto com Python 3.11 (padrÃ£o)
./ds-init.sh meu-projeto

# Ou especificar a versÃ£o do Python
./ds-init.sh meu-projeto 3.12
```

## Script ENV DS / ML

```bash
#!/usr/bin/env bash
# ==============================================================================
# ds-init.sh â€” Inicializador de projetos de Data Science / ML com UV
# Uso: bash ds-init.sh <nome-do-projeto> [versao-python]
# Exemplo: bash ds-init.sh meu-projeto 3.11
# ==============================================================================

set -euo pipefail

# â”€â”€ Cores para output â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

log_info()    { echo -e "${BLUE}[INFO]${NC}  $1"; }
log_success() { echo -e "${GREEN}[OK]${NC}    $1"; }
log_warn()    { echo -e "${YELLOW}[WARN]${NC}  $1"; }
log_error()   { echo -e "${RED}[ERROR]${NC} $1"; exit 1; }

# â”€â”€ ValidaÃ§Ã£o de argumentos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if [ $# -lt 1 ]; then
  echo "Uso: $0 <nome-do-projeto> [versao-python]"
  echo "Exemplo: $0 meu-projeto 3.11"
  exit 1
fi

PROJECT_NAME="$1"
PYTHON_VERSION="${2:-3.11}"

# Validar nome do projeto (apenas letras, nÃºmeros, hÃ­fens e underscores)
if [[ ! "$PROJECT_NAME" =~ ^[a-zA-Z0-9_-]+$ ]]; then
  log_error "Nome do projeto invÃ¡lido. Use apenas letras, nÃºmeros, hÃ­fens e underscores."
fi

# â”€â”€ Verificar se UV estÃ¡ instalado â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ! command -v uv &>/dev/null; then
  log_warn "UV nÃ£o encontrado. Instalando..."
  curl -LsSf https://astral.sh/uv/install.sh | sh
  export PATH="$HOME/.cargo/bin:$PATH"
  log_success "UV instalado com sucesso."
fi

# â”€â”€ Verificar se o diretÃ³rio jÃ¡ existe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if [ -d "$PROJECT_NAME" ]; then
  log_error "DiretÃ³rio '$PROJECT_NAME' jÃ¡ existe. Escolha outro nome."
fi

log_info "Criando projeto '$PROJECT_NAME' com Python $PYTHON_VERSION..."

# â”€â”€ Inicializar projeto com UV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
uv init "$PROJECT_NAME" --python "$PYTHON_VERSION" --no-workspace
cd "$PROJECT_NAME"

# Remover o hello.py gerado pelo uv init
rm -f hello.py

# â”€â”€ Criar estrutura de pastas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log_info "Criando estrutura de diretÃ³rios..."

mkdir -p \
  data/raw \
  data/processed \
  data/external \
  data/interim \
  notebooks/exploratory \
  notebooks/modeling \
  notebooks/reports \
  src/"${PROJECT_NAME//-/_}" \
  models/trained \
  models/checkpoints \
  configs \
  tests \
  reports/figures \
  logs \
  scripts \
  docs

# â”€â”€ Criar arquivos __init__.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SRC_PKG="src/${PROJECT_NAME//-/_}"

touch "${SRC_PKG}/__init__.py"
touch "${SRC_PKG}/data.py"
touch "${SRC_PKG}/features.py"
touch "${SRC_PKG}/models.py"
touch "${SRC_PKG}/evaluate.py"
touch "${SRC_PKG}/utils.py"
touch "${SRC_PKG}/visualize.py"
touch tests/__init__.py
touch tests/test_data.py
touch tests/test_models.py

# â”€â”€ Criar arquivos .gitkeep para pastas vazias â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for dir in data/raw data/processed data/external data/interim \
            models/trained models/checkpoints \
            reports/figures logs; do
  touch "$dir/.gitkeep"
done

# â”€â”€ Criar .gitignore â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log_info "Criando .gitignore..."
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*.pyo
*.pyd
*.so
*.egg
*.egg-info/
dist/
build/
.eggs/

# UV / Ambientes virtuais
.venv/
venv/
env/

# Jupyter
.ipynb_checkpoints/
*.ipynb_checkpoints

# Dados (nÃ£o versionar dados brutos grandes)
data/raw/*
data/processed/*
data/external/*
data/interim/*
!data/**/.gitkeep

# Modelos treinados (geralmente grandes)
models/trained/*
models/checkpoints/*
!models/**/.gitkeep

# Logs e relatÃ³rios
logs/*.log
reports/figures/*
!reports/figures/.gitkeep

# ConfiguraÃ§Ãµes locais e segredos
.env
.env.*
!.env.example
secrets/
*.key

# IDEs
.vscode/
.idea/
*.swp
*.swo
*.DS_Store

# MLflow / DVC / W&B
mlruns/
.dvc/
wandb/
EOF

# â”€â”€ Criar .env.example â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > .env.example << 'EOF'
# Exemplo de variÃ¡veis de ambiente
# Copie para .env e preencha com seus valores reais

PROJECT_NAME=__PROJECT_NAME__

# Caminhos
DATA_DIR=data/
MODELS_DIR=models/
LOGS_DIR=logs/

# Credenciais (nunca commite o .env real)
# AWS_ACCESS_KEY_ID=
# AWS_SECRET_ACCESS_KEY=
# MLFLOW_TRACKING_URI=
# WANDB_API_KEY=
EOF
sed -i "s/__PROJECT_NAME__/$PROJECT_NAME/" .env.example

# â”€â”€ Criar configs/config.yaml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > configs/config.yaml << EOF
project:
  name: $PROJECT_NAME
  version: "0.1.0"

data:
  raw_dir: data/raw
  processed_dir: data/processed
  external_dir: data/external
  interim_dir: data/interim

model:
  random_seed: 42
  test_size: 0.2
  validation_size: 0.1

training:
  epochs: 100
  batch_size: 32
  learning_rate: 0.001
  early_stopping_patience: 10

logging:
  level: INFO
  dir: logs/
EOF

# â”€â”€ Criar Makefile â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > Makefile << 'EOF'
.PHONY: install sync test lint format clean jupyter

install:
	uv sync

sync:
	uv sync --all-extras

test:
	uv run pytest tests/ -v

lint:
	uv run ruff check src/ tests/

format:
	uv run ruff format src/ tests/

clean:
	find . -type f -name "*.pyc" -delete
	find . -type d -name "__pycache__" -exec rm -rf {} +
	find . -type d -name ".ipynb_checkpoints" -exec rm -rf {} +

jupyter:
	uv run jupyter lab notebooks/
EOF

# â”€â”€ Criar notebook de exploraÃ§Ã£o inicial â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log_info "Criando notebook exploratÃ³rio inicial..."
cat > notebooks/exploratory/01_data_exploration.ipynb << 'EOF'
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# ExploraÃ§Ã£o de Dados\n", "\nNotebook inicial para anÃ¡lise exploratÃ³ria (EDA)."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregue seus dados aqui\n",
    "# df = pd.read_csv('../data/raw/dataset.csv')\n",
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF

# â”€â”€ Atualizar pyproject.toml com dependÃªncias de DS/ML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log_info "Configurando pyproject.toml..."
cat > pyproject.toml << EOF
[project]
name = "$PROJECT_NAME"
version = "0.1.0"
description = "Projeto de Data Science / Machine Learning"
readme = "README.md"
requires-python = ">=$PYTHON_VERSION"
dependencies = []

[project.optional-dependencies]
dev = [
    "ipykernel",
    "jupyter",
    "jupyterlab",
    "ruff",
    "pytest",
    "pytest-cov",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.ruff]
line-length = 88
target-version = "py311"

[tool.ruff.lint]
select = ["E", "F", "I", "UP"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
EOF

# â”€â”€ Instalar dependÃªncias de data science â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log_info "Instalando dependÃªncias principais de DS/ML..."

uv add \
  pandas \
  numpy \
  scikit-learn \
  matplotlib \
  seaborn \
  plotly \
  pyyaml \
  python-dotenv \
  tqdm \
  loguru

log_info "Instalando dependÃªncias de desenvolvimento..."

uv add --optional dev \
  ipykernel \
  jupyterlab \
  ruff \
  pytest \
  pytest-cov

# â”€â”€ Atualizar README.md â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log_info "Criando README.md..."
cat > README.md << EOF
# $PROJECT_NAME

> Projeto de Data Science / Machine Learning

## Estrutura do Projeto

\`\`\`
$PROJECT_NAME/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/          # Dados brutos (nÃ£o modificar)
â”‚   â”œâ”€â”€ processed/    # Dados processados e limpos
â”‚   â”œâ”€â”€ interim/      # Dados em transformaÃ§Ã£o intermediÃ¡ria
â”‚   â””â”€â”€ external/     # Dados de fontes externas
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ exploratory/  # EDA e experimentos
â”‚   â”œâ”€â”€ modeling/     # Desenvolvimento de modelos
â”‚   â””â”€â”€ reports/      # Notebooks para relatÃ³rios finais
â”œâ”€â”€ src/$PROJECT_NAME/
â”‚   â”œâ”€â”€ data.py       # IngestÃ£o e processamento de dados
â”‚   â”œâ”€â”€ features.py   # Engenharia de features
â”‚   â”œâ”€â”€ models.py     # DefiniÃ§Ã£o e treino de modelos
â”‚   â”œâ”€â”€ evaluate.py   # MÃ©tricas e avaliaÃ§Ã£o
â”‚   â”œâ”€â”€ visualize.py  # GeraÃ§Ã£o de grÃ¡ficos
â”‚   â””â”€â”€ utils.py      # UtilitÃ¡rios gerais
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ trained/      # Modelos treinados (.pkl, .pt, etc.)
â”‚   â””â”€â”€ checkpoints/  # Checkpoints de treino
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml   # HiperparÃ¢metros e configuraÃ§Ãµes
â”œâ”€â”€ tests/            # Testes unitÃ¡rios
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ figures/      # GrÃ¡ficos e visualizaÃ§Ãµes exportadas
â”œâ”€â”€ logs/             # Logs de treino e execuÃ§Ã£o
â”œâ”€â”€ scripts/          # Scripts utilitÃ¡rios avulsos
â”œâ”€â”€ docs/             # DocumentaÃ§Ã£o adicional
â”œâ”€â”€ .env.example      # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ Makefile          # Atalhos de comandos
â””â”€â”€ pyproject.toml    # DependÃªncias e configuraÃ§Ã£o do projeto
\`\`\`

## ConfiguraÃ§Ã£o do Ambiente

\`\`\`bash
# Instalar dependÃªncias
uv sync

# Instalar com dependÃªncias de desenvolvimento
uv sync --all-extras

# Ativar o ambiente virtual
source .venv/bin/activate
\`\`\`

## Comandos Ãšteis

| Comando          | DescriÃ§Ã£o                     |
|------------------|-------------------------------|
| \`make install\`   | Instala as dependÃªncias       |
| \`make test\`      | Roda os testes                |
| \`make lint\`      | Verifica o cÃ³digo com Ruff    |
| \`make format\`    | Formata o cÃ³digo com Ruff     |
| \`make jupyter\`   | Inicia o JupyterLab           |
| \`make clean\`     | Remove arquivos temporÃ¡rios   |

## Adicionar Novas DependÃªncias

\`\`\`bash
uv add <pacote>                  # DependÃªncia de produÃ§Ã£o
uv add --optional dev <pacote>  # DependÃªncia de desenvolvimento
\`\`\`
EOF

# â”€â”€ Inicializar git e fazer primeiro commit â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log_info "Inicializando repositÃ³rio Git..."

git add -A
git commit -m "feat: inicializa estrutura do projeto $PROJECT_NAME com UV

- Estrutura de diretÃ³rios para DS/ML
- DependÃªncias principais: pandas, numpy, scikit-learn, matplotlib, seaborn, plotly
- DependÃªncias de dev: jupyterlab, ruff, pytest
- ConfiguraÃ§Ãµes: pyproject.toml, configs/config.yaml, Makefile
- Notebook exploratÃ³rio inicial"

# â”€â”€ Resumo final â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
echo ""
echo -e "${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${GREEN}  âœ…  Projeto '$PROJECT_NAME' criado com sucesso!${NC}"
echo -e "${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""
echo -e "  ğŸ“  DiretÃ³rio : ${BLUE}$(pwd)${NC}"
echo -e "  ğŸ  Python    : ${BLUE}$PYTHON_VERSION${NC}"
echo -e "  ğŸ“¦  Gerenciador: ${BLUE}UV${NC}"
echo ""
echo -e "  ${YELLOW}PrÃ³ximos passos:${NC}"
echo -e "  1. ${GREEN}cd $PROJECT_NAME${NC}"
echo -e "  2. ${GREEN}cp .env.example .env${NC}  â†’ configure suas variÃ¡veis"
echo -e "  3. ${GREEN}uv sync${NC}               â†’ instala as dependÃªncias"
echo -e "  4. ${GREEN}make jupyter${NC}           â†’ abre o JupyterLab"
echo ""

```

## **O que o script faz**

O script cobre todo o ciclo desde a inicializaÃ§Ã£o atÃ© o primeiro commit:

**VerificaÃ§Ãµes iniciais**

- Detecta se UV estÃ¡ instalado e, se nÃ£o estiver, instala automaticamente
- Valida o nome do projeto e evita sobrescrever pastas existentes

**Estrutura de pastas criada**

- `data/`Â com subpastasÂ `raw`,Â `processed`,Â `interim`Â eÂ `external`
- `notebooks/`Â dividido emÂ `exploratory`,Â `modeling`Â eÂ `reports`
- `src/<projeto>/`Â com mÃ³dulos separados:Â `data.py`,Â `features.py`,Â `models.py`,Â `evaluate.py`,Â `visualize.py`,Â `utils.py`
- `models/`,Â `configs/`,Â `tests/`,Â `reports/figures/`,Â `logs/`,Â `scripts/`,Â `docs/`

**Arquivos gerados**

- `pyproject.toml`Â configurado com dependÃªncias de DS/ML viaÂ `uv add`
- `configs/config.yaml`Â com hiperparÃ¢metros, caminhos e seed padrÃ£o
- `.gitignore`Â completo para Python, UV, Jupyter, dados e modelos
- `.env.example`Â para gerenciamento de segredos
- `Makefile`Â com atalhos paraÂ `install`,Â `test`,Â `lint`,Â `format`Â eÂ `jupyter`
- Notebook exploratÃ³rio inicial emÂ `notebooks/exploratory/`
- `README.md`Â completo com a estrutura documentada

**DependÃªncias instaladas automaticamente**

- ProduÃ§Ã£o:Â `pandas`,Â `numpy`,Â `scikit-learn`,Â `matplotlib`,Â `seaborn`,Â `plotly`,Â `pyyaml`,Â `python-dotenv`,Â `tqdm`,Â `loguru`
- Dev:Â `jupyterlab`,Â `ruff`,Â `pytest`,Â `pytest-cov`

## Script ENV DS / FastAPI / Docker

```bash
#!/usr/bin/env bash
# ==============================================================================
# init-project.sh â€” Cria estrutura completa DS + API para projetos MLOps
#
# Uso:
#   bash init-project.sh <nome-projeto> [diretÃ³rio-destino] [python-version]
#
# Exemplos:
#   bash init-project.sh fraud-detector
#   bash init-project.sh fraud-detector ~/projetos
#   bash init-project.sh fraud-detector ~/projetos 3.12
# ==============================================================================

set -euo pipefail

GREEN='\033[0;32m'; BLUE='\033[0;34m'; YELLOW='\033[1;33m'; NC='\033[0m'
log_info()    { echo -e "${BLUE}[INFO]${NC}  $1"; }
log_success() { echo -e "${GREEN}[OK]${NC}    $1"; }
log_step()    { echo -e "\n${YELLOW}â•â• $1${NC}"; }

# â”€â”€ Argumentos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if [ $# -lt 1 ]; then
  echo "Uso: $0 <nome-projeto> [diretÃ³rio-destino] [python-version]"
  exit 1
fi

PROJECT_NAME="$1"
TARGET_DIR="${2:-$PWD}"
PYTHON_VERSION="${3:-3.11}"

if [[ ! "$PROJECT_NAME" =~ ^[a-zA-Z0-9_-]+$ ]]; then
  echo "Nome invÃ¡lido. Use apenas letras, nÃºmeros, hÃ­fens e underscores."; exit 1
fi

TARGET_DIR="$(realpath "$TARGET_DIR")"
mkdir -p "$TARGET_DIR"

DS_DIR="$TARGET_DIR/${PROJECT_NAME}-ds"
API_DIR="$TARGET_DIR/${PROJECT_NAME}-api"
PKG_NAME="${PROJECT_NAME//-/_}"

[ -d "$DS_DIR" ]  && echo "DiretÃ³rio '$DS_DIR' jÃ¡ existe."  && exit 1
[ -d "$API_DIR" ] && echo "DiretÃ³rio '$API_DIR' jÃ¡ existe." && exit 1

# â”€â”€ Verificar UV â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if ! command -v uv &>/dev/null; then
  log_info "Instalando UV..."
  curl -LsSf https://astral.sh/uv/install.sh | sh
  export PATH="$HOME/.cargo/bin:$PATH"
fi

# ==============================================================================
# PARTE 1 â€” REPO DE DATA SCIENCE
# ==============================================================================
log_step "Criando repo de Data Science: $DS_DIR"

uv init "$DS_DIR" --python "$PYTHON_VERSION" --no-workspace
cd "$DS_DIR"
rm -f hello.py

# â”€â”€ Estrutura de pastas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mkdir -p \
  data/{raw,processed,interim,external} \
  notebooks/{exploratory,modeling,reports} \
  src/"$PKG_NAME" \
  models/{trained,checkpoints} \
  configs \
  tests \
  reports/figures \
  scripts \
  logs \
  docs

# â”€â”€ MÃ³dulos do pacote src/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
touch src/"$PKG_NAME"/__init__.py
for module in data features train evaluate visualize utils; do
  touch src/"$PKG_NAME"/${module}.py
done
touch tests/__init__.py tests/test_data.py tests/test_train.py

# â”€â”€ .gitkeeps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
for d in data/raw data/processed data/interim data/external \
          models/trained models/checkpoints reports/figures logs; do
  touch "$d/.gitkeep"
done

# â”€â”€ configs/config.yaml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > configs/config.yaml << EOF
project:
  name: ${PROJECT_NAME}
  version: "0.1.0"

data:
  raw_dir: data/raw
  processed_dir: data/processed
  interim_dir: data/interim
  external_dir: data/external

model:
  random_seed: 42
  test_size: 0.2
  validation_size: 0.1
  max_depth: 5

training:
  epochs: 100
  batch_size: 32
  learning_rate: 0.001
  early_stopping_patience: 10

mlflow:
  tracking_uri: "http://localhost:5000"
  experiment_name: "${PROJECT_NAME}"
  registered_model_name: "${PKG_NAME}_model"
EOF

# â”€â”€ src/train.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > src/"$PKG_NAME"/train.py << 'PYEOF'
import mlflow
import mlflow.sklearn
import yaml
import joblib
from pathlib import Path
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score
import pandas as pd

def load_config(path: str = "configs/config.yaml") -> dict:
    with open(path) as f:
        return yaml.safe_load(f)

def train(config_path: str = "configs/config.yaml"):
    cfg = load_config(config_path)
    mlflow.set_tracking_uri(cfg["mlflow"]["tracking_uri"])
    mlflow.set_experiment(cfg["mlflow"]["experiment_name"])

    # --- Substitua pelo carregamento real dos seus dados ---
    # df = pd.read_csv("data/processed/dataset.csv")
    # X, y = df.drop("target", axis=1), df["target"]
    # X_train, X_test, y_train, y_test = train_test_split(
    #     X, y,
    #     test_size=cfg["model"]["test_size"],
    #     random_state=cfg["model"]["random_seed"],
    # )

    with mlflow.start_run():
        params = {
            "n_estimators": 100,
            "max_depth": cfg["model"]["max_depth"],
            "random_state": cfg["model"]["random_seed"],
        }
        mlflow.log_params(params)
        mlflow.log_param("test_size", cfg["model"]["test_size"])

        model = RandomForestClassifier(**params)
        # model.fit(X_train, y_train)
        # preds = model.predict(X_test)
        # f1  = f1_score(y_test, preds, average="weighted")
        # acc = accuracy_score(y_test, preds)
        # mlflow.log_metric("f1_score", f1)
        # mlflow.log_metric("accuracy", acc)

        mlflow.sklearn.log_model(
            model,
            artifact_path="model",
            registered_model_name=cfg["mlflow"]["registered_model_name"],
        )

        Path("models/trained").mkdir(exist_ok=True)
        joblib.dump(model, "models/trained/model.pkl")
        mlflow.log_artifact("models/trained/model.pkl")

    print("Treino concluÃ­do e modelo registrado no MLflow.")

if __name__ == "__main__":
    train()
PYEOF

# â”€â”€ src/data.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > src/"$PKG_NAME"/data.py << 'PYEOF'
import pandas as pd
from pathlib import Path

def load_raw(filename: str, data_dir: str = "data/raw") -> pd.DataFrame:
    """Carrega dados brutos do disco."""
    path = Path(data_dir) / filename
    return pd.read_csv(path)

def save_processed(df: pd.DataFrame, filename: str, data_dir: str = "data/processed"):
    """Salva dados processados no disco."""
    Path(data_dir).mkdir(parents=True, exist_ok=True)
    df.to_csv(Path(data_dir) / filename, index=False)
    print(f"Dados salvos em {data_dir}/{filename}")
PYEOF

# â”€â”€ src/features.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > src/"$PKG_NAME"/features.py << 'PYEOF'
import pandas as pd
from sklearn.preprocessing import StandardScaler

def build_features(df: pd.DataFrame) -> pd.DataFrame:
    """Aplica transformaÃ§Ãµes e engenharia de features."""
    df = df.copy()
    # Exemplo: normalizar colunas numÃ©ricas
    # num_cols = df.select_dtypes(include="number").columns
    # scaler = StandardScaler()
    # df[num_cols] = scaler.fit_transform(df[num_cols])
    return df
PYEOF

# â”€â”€ src/evaluate.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > src/"$PKG_NAME"/evaluate.py << 'PYEOF'
import pandas as pd
from sklearn.metrics import (
    accuracy_score,
    f1_score,
    precision_score,
    recall_score,
    classification_report,
)

def evaluate_model(model, X_test, y_test) -> dict:
    """Retorna dicionÃ¡rio com mÃ©tricas de avaliaÃ§Ã£o."""
    preds = model.predict(X_test)
    metrics = {
        "accuracy":  accuracy_score(y_test, preds),
        "f1_score":  f1_score(y_test, preds, average="weighted"),
        "precision": precision_score(y_test, preds, average="weighted"),
        "recall":    recall_score(y_test, preds, average="weighted"),
    }
    print(classification_report(y_test, preds))
    return metrics
PYEOF

# â”€â”€ src/utils.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > src/"$PKG_NAME"/utils.py << 'PYEOF'
import yaml
import logging
from pathlib import Path

def load_config(path: str = "configs/config.yaml") -> dict:
    with open(path) as f:
        return yaml.safe_load(f)

def get_logger(name: str, log_dir: str = "logs") -> logging.Logger:
    Path(log_dir).mkdir(exist_ok=True)
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(f"{log_dir}/{name}.log"),
        ],
    )
    return logging.getLogger(name)
PYEOF

# â”€â”€ pyproject.toml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > pyproject.toml << EOF
[project]
name = "${PROJECT_NAME}-ds"
version = "0.1.0"
description = "Data Science / ML â€” ${PROJECT_NAME}"
readme = "README.md"
requires-python = ">=${PYTHON_VERSION}"
dependencies = []

[project.optional-dependencies]
dev = ["jupyterlab", "ruff", "pytest", "pytest-cov", "ipykernel"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/${PKG_NAME}"]

[tool.ruff]
line-length = 88
target-version = "py311"

[tool.ruff.lint]
select = ["E", "F", "I", "UP"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
EOF

# â”€â”€ .gitignore â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*.pyo
*.pyd
.venv/
*.egg-info/
dist/
build/

# Jupyter
.ipynb_checkpoints/

# Dados (versionados pelo DVC)
data/raw/*
data/processed/*
data/interim/*
data/external/*
!data/**/.gitkeep

# Modelos treinados
models/trained/*
models/checkpoints/*
!models/**/.gitkeep

# Logs e relatÃ³rios
logs/*.log
reports/figures/*
!reports/figures/.gitkeep

# MLflow
mlruns/

# DVC
.dvc/tmp
.dvc/cache

# Segredos
.env
.env.*
!.env.example
EOF

# â”€â”€ .env.example â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > .env.example << EOF
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=${PROJECT_NAME}
EOF

# â”€â”€ Makefile â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > Makefile << EOF
.PHONY: install train test lint format jupyter mlflow-ui dvc-push dvc-pull dvc-repro clean

install:
	uv sync --all-extras

train:
	uv run python -m src.${PKG_NAME}.train

test:
	uv run pytest tests/ -v --cov=src --cov-report=term-missing

lint:
	uv run ruff check src/ tests/

format:
	uv run ruff format src/ tests/

jupyter:
	uv run jupyter lab notebooks/

mlflow-ui:
	uv run mlflow ui --port 5000

dvc-push:
	uv run dvc push

dvc-pull:
	uv run dvc pull

dvc-repro:
	uv run dvc repro

clean:
	find . -type f -name "*.pyc" -delete
	find . -type d -name "__pycache__" -exec rm -rf {} +
	find . -type d -name ".ipynb_checkpoints" -exec rm -rf {} +
EOF

# â”€â”€ Notebook exploratÃ³rio â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > notebooks/exploratory/01_data_exploration.ipynb << 'EOF'
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# ExploraÃ§Ã£o de Dados\n", "\nNotebook inicial para anÃ¡lise exploratÃ³ria (EDA)."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregue seus dados aqui\n",
    "# df = pd.read_csv('../../data/raw/dataset.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.info()\n",
    "# df.describe()\n",
    "# df.isnull().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.11.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
EOF

# â”€â”€ README.md â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > README.md << EOF
# ${PROJECT_NAME} â€” Data Science

RepositÃ³rio de experimentaÃ§Ã£o, treinamento e versionamento de dados do projeto **${PROJECT_NAME}**.
O modelo treinado Ã© publicado no MLflow Model Registry e consumido pelo repo \`${PROJECT_NAME}-api\`.

---

## PrÃ©-requisitos

- [uv](https://github.com/astral-sh/uv) instalado
- [DVC](https://dvc.org/) (instalado via \`uv add dvc\`)
- Acesso ao MLflow Tracking Server (local ou remoto)

---

## Setup

\`\`\`bash
# 1. Instalar todas as dependÃªncias (produÃ§Ã£o + dev)
uv sync --all-extras

# 2. Configurar variÃ¡veis de ambiente
cp .env.example .env
# Edite .env com suas configuraÃ§Ãµes reais

# 3. Configurar remote DVC (escolha um)
uv run dvc remote add -d local_remote /tmp/dvc-storage          # local
uv run dvc remote add -d s3_remote s3://meu-bucket/dvc-data     # AWS S3
uv run dvc remote add -d gcs_remote gs://meu-bucket/dvc-data    # GCS
\`\`\`

---

## Estrutura de Pastas

\`\`\`
${PROJECT_NAME}-ds/
â”‚
â”œâ”€â”€ data/                        # Dados do projeto (NÃƒO commitados â€” gerenciados pelo DVC)
â”‚   â”œâ”€â”€ raw/                     # Dados brutos originais, nunca modificados
â”‚   â”œâ”€â”€ processed/               # Dados limpos, transformados e prontos para treino
â”‚   â”œâ”€â”€ interim/                 # Dados em transformaÃ§Ãµes intermediÃ¡rias
â”‚   â””â”€â”€ external/                # Dados de fontes externas (APIs, parceiros, etc.)
â”‚
â”œâ”€â”€ notebooks/                   # Jupyter Notebooks organizados por fase
â”‚   â”œâ”€â”€ exploratory/             # EDA livre â€” anÃ¡lise inicial, hipÃ³teses, visualizaÃ§Ãµes
â”‚   â”‚   â””â”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ modeling/                # Experimentos de modelagem, comparaÃ§Ã£o de algoritmos
â”‚   â””â”€â”€ reports/                 # Notebooks finais e limpos para apresentaÃ§Ã£o
â”‚
â”œâ”€â”€ src/${PKG_NAME}/             # Pacote Python principal (cÃ³digo reutilizÃ¡vel)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data.py                  # FunÃ§Ãµes de ingestÃ£o, leitura e salvamento de dados
â”‚   â”œâ”€â”€ features.py              # Engenharia de features e transformaÃ§Ãµes
â”‚   â”œâ”€â”€ train.py                 # Pipeline de treino com logging no MLflow
â”‚   â”œâ”€â”€ evaluate.py              # CÃ¡lculo e logging de mÃ©tricas de avaliaÃ§Ã£o
â”‚   â”œâ”€â”€ visualize.py             # GeraÃ§Ã£o de grÃ¡ficos e visualizaÃ§Ãµes exportÃ¡veis
â”‚   â””â”€â”€ utils.py                 # UtilitÃ¡rios: logger, carregamento de config, etc.
â”‚
â”œâ”€â”€ models/                      # Artefatos de modelo (NÃƒO commitados no Git)
â”‚   â”œâ”€â”€ trained/                 # Modelos treinados exportados (.pkl, .pt, .onnx)
â”‚   â””â”€â”€ checkpoints/             # Checkpoints intermediÃ¡rios de treino (ex: epochs)
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml              # HiperparÃ¢metros, caminhos e configuraÃ§Ãµes do projeto
â”‚
â”œâ”€â”€ tests/                       # Testes unitÃ¡rios do pacote src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_data.py             # Testa funÃ§Ãµes de ingestÃ£o e processamento
â”‚   â””â”€â”€ test_train.py            # Testa o pipeline de treino
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ figures/                 # GrÃ¡ficos e imagens exportadas para relatÃ³rios
â”‚
â”œâ”€â”€ scripts/                     # Scripts avulsos de automaÃ§Ã£o e utilitÃ¡rios
â”œâ”€â”€ logs/                        # Logs de execuÃ§Ã£o e treino (gerados em runtime)
â”œâ”€â”€ docs/                        # DocumentaÃ§Ã£o adicional do projeto
â”‚
â”œâ”€â”€ dvc.yaml                     # DefiniÃ§Ã£o do pipeline reproduzÃ­vel (stages)
â”œâ”€â”€ dvc.lock                     # Lock file do DVC (commitado no Git)
â”œâ”€â”€ .dvc/config                  # ConfiguraÃ§Ã£o do remote DVC (commitado no Git)
â”‚
â”œâ”€â”€ .env.example                 # Exemplo de variÃ¡veis de ambiente
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Makefile                     # Atalhos de comandos
â””â”€â”€ pyproject.toml               # DependÃªncias e configuraÃ§Ã£o do projeto (UV)
\`\`\`

---

## Comandos

| Comando | DescriÃ§Ã£o |
|---|---|
| \`make install\` | Instala todas as dependÃªncias |
| \`make train\` | Executa o pipeline de treino e loga no MLflow |
| \`make mlflow-ui\` | Abre o MLflow UI em \`http://localhost:5000\` |
| \`make jupyter\` | Abre o JupyterLab em \`notebooks/\` |
| \`make dvc-repro\` | Executa o pipeline DVC (sÃ³ roda o que mudou) |
| \`make dvc-push\` | Envia dados e modelos para o remote DVC |
| \`make dvc-pull\` | Baixa dados e modelos do remote DVC |
| \`make test\` | Roda os testes com cobertura |
| \`make lint\` | Verifica o cÃ³digo com Ruff |
| \`make format\` | Formata o cÃ³digo com Ruff |
| \`make clean\` | Remove arquivos temporÃ¡rios e cache |

---

## Fluxo de Trabalho

\`\`\`
1. Dados brutos em data/raw/  (versionados pelo DVC)
        â”‚
        â–¼
2. Limpeza e feature engineering  (src/data.py, src/features.py)
        â”‚
        â–¼
3. Treino com logging  (make train â†’ MLflow Experiment)
        â”‚
        â–¼
4. AvaliaÃ§Ã£o e comparaÃ§Ã£o  (MLflow UI â†’ http://localhost:5000)
        â”‚
        â–¼
5. PromoÃ§Ã£o do melhor modelo  (MLflow UI â†’ "Production")
        â”‚
        â–¼
6. API consome automaticamente  (repo ${PROJECT_NAME}-api)
\`\`\`

---

## Adicionar DependÃªncias

\`\`\`bash
uv add <pacote>                   # dependÃªncia de produÃ§Ã£o
uv add --optional dev <pacote>   # dependÃªncia de desenvolvimento
\`\`\`

---

## IntegraÃ§Ã£o com a API

ApÃ³s promover um modelo para **Production** no MLflow UI,
o repo \`${PROJECT_NAME}-api\` irÃ¡ carregar automaticamente o novo modelo
na prÃ³xima inicializaÃ§Ã£o do servidor.
EOF

# â”€â”€ Instalar dependÃªncias DS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log_info "Instalando dependÃªncias DS..."
uv add \
  pandas numpy scikit-learn matplotlib seaborn plotly \
  pyyaml python-dotenv tqdm loguru joblib mlflow dvc

log_info "Instalando dependÃªncias de desenvolvimento DS..."
uv add --optional dev jupyterlab ruff pytest pytest-cov ipykernel

# â”€â”€ Inicializar DVC â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log_info "Inicializando DVC..."
uv run dvc init

# â”€â”€ Git commit inicial â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
git add -A
git commit -m "feat: inicializa projeto DS ${PROJECT_NAME}

- src layout com hatchling configurado (packages = src/${PKG_NAME})
- DependÃªncias: pandas, numpy, scikit-learn, mlflow, dvc
- MÃ³dulos: data, features, train, evaluate, visualize, utils
- DVC inicializado
- Configs, Makefile, notebook exploratÃ³rio, README detalhado"

log_success "Repo DS criado em $DS_DIR"

# ==============================================================================
# PARTE 2 â€” REPO DE API (FastAPI)
# ==============================================================================
log_step "Criando repo de API: $API_DIR"

cd "$TARGET_DIR"
uv init "$API_DIR" --python "$PYTHON_VERSION" --no-workspace
cd "$API_DIR"
rm -f hello.py

# â”€â”€ Estrutura de pastas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
mkdir -p \
  app/{routers,schemas,services,middleware} \
  tests \
  scripts \
  models

touch models/.gitkeep

# â”€â”€ __init__.py em todos os subpacotes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
touch app/__init__.py
touch app/routers/__init__.py
touch app/schemas/__init__.py
touch app/services/__init__.py
touch app/middleware/__init__.py
touch tests/__init__.py

# â”€â”€ app/main.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > app/main.py << 'PYEOF'
from contextlib import asynccontextmanager
from fastapi import FastAPI
from app.routers import predict
from app.services.model import ModelService

model_service = ModelService()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Carrega o modelo na inicializaÃ§Ã£o e libera recursos no shutdown."""
    model_service.load()
    yield
    model_service.unload()

app = FastAPI(
    title="ML Model API",
    description="API de inferÃªncia para modelos de ML",
    version="0.1.0",
    lifespan=lifespan,
)

app.include_router(predict.router, prefix="/api/v1")

@app.get("/health", tags=["Health"])
def health():
    return {"status": "ok", "model_loaded": model_service.is_loaded}
PYEOF

# â”€â”€ app/services/model.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Parte 1 (EOF): injeta DEFAULT_MODEL_NAME com $PKG_NAME expandido pelo bash
cat > app/services/model.py << EOF
import os
import joblib
import mlflow.pyfunc
from pathlib import Path

DEFAULT_MODEL_NAME = "${PKG_NAME}_model"
EOF

# Parte 2 ('PYEOF'): cÃ³digo Python puro, sem expansÃ£o bash
cat >> app/services/model.py << 'PYEOF'

class ModelService:
    def __init__(self):
        self.model = None

    @property
    def is_loaded(self) -> bool:
        return self.model is not None

    def load(self):
        tracking_uri = os.getenv("MLFLOW_TRACKING_URI")
        model_name   = os.getenv("MLFLOW_MODEL_NAME", DEFAULT_MODEL_NAME)
        model_stage  = os.getenv("MLFLOW_MODEL_STAGE", "Production")

        if tracking_uri:
            # ProduÃ§Ã£o: carrega do MLflow Model Registry
            mlflow.set_tracking_uri(tracking_uri)
            self.model = mlflow.pyfunc.load_model(
                f"models:/{model_name}/{model_stage}"
            )
            print(f"Modelo '{model_name}/{model_stage}' carregado do MLflow.")
        else:
            # Desenvolvimento: fallback para modelo local
            local_path = Path(os.getenv("LOCAL_MODEL_PATH", "models/model.pkl"))
            if local_path.exists():
                self.model = joblib.load(local_path)
                print(f"Modelo carregado localmente de '{local_path}'.")
            else:
                print(
                    "[WARN] Nenhum modelo encontrado. "
                    "Configure MLFLOW_TRACKING_URI ou LOCAL_MODEL_PATH."
                )

    def unload(self):
        self.model = None

    def predict(self, features: list[float]):
        if not self.is_loaded:
            raise RuntimeError("Modelo nÃ£o estÃ¡ carregado.")
        import pandas as pd
        df = pd.DataFrame([features])
        return self.model.predict(df).tolist()[0]
PYEOF

# â”€â”€ app/schemas/predict.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > app/schemas/predict.py << 'PYEOF'
from pydantic import BaseModel, Field
from typing import Any

class PredictRequest(BaseModel):
    features: list[float] = Field(..., description="Vetor de features de entrada")

    model_config = {
        "json_schema_extra": {"example": {"features": [1.2, 3.4, 5.6]}}
    }

class PredictResponse(BaseModel):
    prediction: Any
    probability: float | None = None
    model_version: str = "unknown"
PYEOF

# â”€â”€ app/routers/predict.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > app/routers/predict.py << 'PYEOF'
from fastapi import APIRouter, HTTPException
from app.schemas.predict import PredictRequest, PredictResponse
from app.services.model import ModelService

router = APIRouter(tags=["Inference"])
_svc = ModelService()

@router.post("/predict", response_model=PredictResponse)
def predict(req: PredictRequest):
    try:
        result = _svc.predict(req.features)
        return PredictResponse(prediction=result)
    except RuntimeError as e:
        raise HTTPException(status_code=503, detail=str(e))
PYEOF

# â”€â”€ tests/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > tests/test_health.py << 'PYEOF'
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_health_returns_ok():
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json()["status"] == "ok"

def test_health_has_model_loaded_field():
    response = client.get("/health")
    assert "model_loaded" in response.json()
PYEOF

cat > tests/test_predict.py << 'PYEOF'
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

def test_predict_without_model_returns_503():
    response = client.post(
        "/api/v1/predict",
        json={"features": [1.0, 2.0, 3.0]},
    )
    assert response.status_code == 503

def test_predict_payload_validation():
    # Features deve ser lista de floats
    response = client.post(
        "/api/v1/predict",
        json={"features": "invalido"},
    )
    assert response.status_code == 422
PYEOF

# â”€â”€ pyproject.toml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > pyproject.toml << EOF
[project]
name = "${PROJECT_NAME}-api"
version = "0.1.0"
description = "API FastAPI para serving â€” ${PROJECT_NAME}"
readme = "README.md"
requires-python = ">=${PYTHON_VERSION}"
dependencies = []

[project.optional-dependencies]
dev = ["pytest", "httpx", "ruff"]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["app"]

[tool.ruff]
line-length = 88
target-version = "py311"

[tool.ruff.lint]
select = ["E", "F", "I", "UP"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
EOF

# â”€â”€ .env.example â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > .env.example << EOF
# ProduÃ§Ã£o: carrega do MLflow Model Registry
MLFLOW_TRACKING_URI=http://mlflow:5000
MLFLOW_MODEL_NAME=${PKG_NAME}_model
MLFLOW_MODEL_STAGE=Production

# Desenvolvimento: usa modelo local (.pkl)
# LOCAL_MODEL_PATH=models/model.pkl
EOF

# â”€â”€ .gitignore â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*.pyo
.venv/
*.egg-info/
dist/
build/

# Modelos (nÃ£o versionar no Git â€” usar DVC ou MLflow)
models/*.pkl
models/*.pt
models/*.onnx
!models/.gitkeep

# Segredos
.env
.env.*
!.env.example
EOF

# â”€â”€ Dockerfile â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > Dockerfile << 'EOF'
FROM python:3.11-slim

WORKDIR /app

# Copia o binÃ¡rio do UV da imagem oficial
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# Instala dependÃªncias (aproveita cache do Docker em rebuilds)
COPY pyproject.toml uv.lock* ./
RUN uv sync --frozen --no-dev

# Copia o cÃ³digo da aplicaÃ§Ã£o
COPY app/ ./app/

EXPOSE 8000
CMD ["uv", "run", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
EOF

# â”€â”€ docker-compose.yml â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > docker-compose.yml << EOF
services:
  api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_MODEL_NAME=${PKG_NAME}_model
      - MLFLOW_MODEL_STAGE=Production
    depends_on:
      - mlflow
    restart: unless-stopped

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5000:5000"
    command: mlflow server --host 0.0.0.0 --port 5000
    volumes:
      - mlflow_data:/mlflow
    restart: unless-stopped

volumes:
  mlflow_data:
EOF

# â”€â”€ Makefile â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > Makefile << 'EOF'
.PHONY: install dev test lint format build up down logs

install:
	uv sync --all-extras

dev:
	uv run uvicorn app.main:app --reload --port 8000

test:
	uv run pytest tests/ -v

lint:
	uv run ruff check app/ tests/

format:
	uv run ruff format app/ tests/

build:
	docker build -t $(shell basename $(CURDIR)):latest .

up:
	docker compose up --build

down:
	docker compose down

logs:
	docker compose logs -f api
EOF

# â”€â”€ README.md â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
cat > README.md << EOF
# ${PROJECT_NAME} â€” API

API FastAPI para servir o modelo treinado no repo \`${PROJECT_NAME}-ds\`.
O modelo Ã© carregado automaticamente do **MLflow Model Registry** na inicializaÃ§Ã£o.

---

## PrÃ©-requisitos

- [uv](https://github.com/astral-sh/uv) instalado
- Docker e Docker Compose (para deploy completo)
- MLflow Tracking Server rodando (ou modelo local em \`models/\`)

---

## Setup

\`\`\`bash
# 1. Instalar dependÃªncias
uv sync

# 2. Configurar variÃ¡veis de ambiente
cp .env.example .env
# Edite .env com suas configuraÃ§Ãµes

# OpÃ§Ã£o A â€” servidor local com modelo do MLflow
make dev

# OpÃ§Ã£o B â€” Docker Compose (API + MLflow juntos)
make up
\`\`\`

---

## Estrutura de Pastas

\`\`\`
${PROJECT_NAME}-api/
â”‚
â”œâ”€â”€ app/                         # Pacote principal da aplicaÃ§Ã£o FastAPI
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                  # Entrypoint: cria o app, registra routers, lifespan
â”‚   â”‚
â”‚   â”œâ”€â”€ routers/                 # Rotas organizadas por domÃ­nio
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ predict.py           # POST /api/v1/predict â€” endpoint de inferÃªncia
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/                 # Modelos Pydantic de request/response
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ predict.py           # PredictRequest, PredictResponse
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                # LÃ³gica de negÃ³cio desacoplada dos routers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ model.py             # ModelService: carrega e executa o modelo
â”‚   â”‚
â”‚   â””â”€â”€ middleware/              # Middlewares customizados (auth, logging, CORS, etc.)
â”‚       â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ tests/                       # Testes automatizados com TestClient do FastAPI
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_health.py           # Testa GET /health
â”‚   â””â”€â”€ test_predict.py          # Testa POST /api/v1/predict (com e sem modelo)
â”‚
â”œâ”€â”€ models/                      # Modelo local para desenvolvimento sem MLflow
â”‚   â””â”€â”€ model.pkl                # Copiado do repo DS ou baixado via DVC
â”‚
â”œâ”€â”€ scripts/                     # Scripts auxiliares (ex: smoke test, warmup)
â”‚
â”œâ”€â”€ Dockerfile                   # Imagem de produÃ§Ã£o com UV
â”œâ”€â”€ docker-compose.yml           # Orquestra API + MLflow localmente
â”‚
â”œâ”€â”€ .env.example                 # VariÃ¡veis de ambiente necessÃ¡rias
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Makefile                     # Atalhos de comandos
â””â”€â”€ pyproject.toml               # DependÃªncias e configuraÃ§Ã£o (UV + Hatchling)
\`\`\`

---

## Endpoints

| MÃ©todo | Rota | DescriÃ§Ã£o |
|---|---|---|
| \`GET\` | \`/health\` | Status da API e se o modelo estÃ¡ carregado |
| \`POST\` | \`/api/v1/predict\` | Recebe features e retorna prediÃ§Ã£o |
| \`GET\` | \`/docs\` | Swagger UI interativo |
| \`GET\` | \`/redoc\` | DocumentaÃ§Ã£o ReDoc |

### Exemplo de chamada

\`\`\`bash
curl -X POST http://localhost:8000/api/v1/predict \\
     -H "Content-Type: application/json" \\
     -d '{"features": [1.2, 3.4, 5.6]}'
\`\`\`

### Resposta esperada

\`\`\`json
{
  "prediction": 1,
  "probability": 0.87,
  "model_version": "unknown"
}
\`\`\`

---

## Comandos

| Comando | DescriÃ§Ã£o |
|---|---|
| \`make dev\` | Servidor local com hot-reload (porta 8000) |
| \`make test\` | Roda os testes |
| \`make lint\` | Verifica o cÃ³digo com Ruff |
| \`make format\` | Formata o cÃ³digo com Ruff |
| \`make up\` | Sobe API + MLflow com Docker Compose |
| \`make down\` | Para os containers |
| \`make build\` | Build da imagem Docker |
| \`make logs\` | Exibe logs do container da API |

---

## Carregamento do Modelo

O \`ModelService\` segue esta lÃ³gica na inicializaÃ§Ã£o:

\`\`\`
MLFLOW_TRACKING_URI definido?
    â”œâ”€â”€ SIM â†’ carrega models:/<MLFLOW_MODEL_NAME>/<MLFLOW_MODEL_STAGE> do Registry
    â””â”€â”€ NÃƒO â†’ carrega arquivo local em LOCAL_MODEL_PATH (padrÃ£o: models/model.pkl)
\`\`\`

Para desenvolvimento sem MLflow, exporte o modelo no repo DS e copie para \`models/\`:

\`\`\`bash
# No repo DS
cp models/trained/model.pkl ../$(echo ${PROJECT_NAME})-api/models/model.pkl
\`\`\`

---

## IntegraÃ§Ã£o com DS

O fluxo de integraÃ§Ã£o entre os dois repos Ã©:

\`\`\`
[DS] make train
      â””â”€â”€ loga experimento no MLflow
            â””â”€â”€ registra modelo em "Staging"
                  â””â”€â”€ [aprovaÃ§Ã£o manual no MLflow UI]
                        â””â”€â”€ modelo promovido para "Production"
                              â””â”€â”€ [API reinicia]
                                    â””â”€â”€ ModelService.load() puxa "Production" automaticamente
\`\`\`

---

## Adicionar DependÃªncias

\`\`\`bash
uv add <pacote>                   # dependÃªncia de produÃ§Ã£o
uv add --optional dev <pacote>   # dependÃªncia de desenvolvimento
\`\`\`
EOF

# â”€â”€ Instalar dependÃªncias API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log_info "Instalando dependÃªncias API..."
uv add fastapi "uvicorn[standard]" pydantic mlflow joblib numpy pandas python-dotenv

log_info "Instalando dependÃªncias de desenvolvimento API..."
uv add --optional dev pytest httpx ruff

# â”€â”€ Git commit inicial â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Commita APÃ“S o uv add para incluir pyproject.toml e uv.lock atualizados
git add -A
git commit -m "feat: inicializa API FastAPI para ${PROJECT_NAME}

- FastAPI com lifespan para carregamento de modelo
- ModelService com suporte a MLflow Registry e fallback local
- Schemas Pydantic: PredictRequest, PredictResponse
- Dockerfile + docker-compose com MLflow
- Testes de health, predict e validaÃ§Ã£o de payload
- hatchling configurado para pacote app/
- README detalhado com estrutura e fluxo de integraÃ§Ã£o"

log_success "Repo API criado em $API_DIR"

# ==============================================================================
# RESUMO FINAL
# ==============================================================================
echo ""
echo -e "${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${GREEN}  âœ…  Projeto '${PROJECT_NAME}' criado com sucesso!${NC}"
echo -e "${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""
echo -e "  ğŸ“  DS  â†’ ${BLUE}${DS_DIR}${NC}"
echo -e "  ğŸ“  API â†’ ${BLUE}${API_DIR}${NC}"
echo ""
echo -e "  ${YELLOW}PrÃ³ximos passos:${NC}"
echo ""
echo -e "  ${YELLOW}# 1. Configure o DVC remote no repo DS:${NC}"
echo -e "  cd ${DS_DIR}"
echo -e "  ${GREEN}uv run dvc remote add -d myremote s3://meu-bucket/dvc${NC}"
echo ""
echo -e "  ${YELLOW}# 2. Treine e registre o modelo:${NC}"
echo -e "  ${GREEN}make mlflow-ui${NC}   # em outro terminal â†’ http://localhost:5000"
echo -e "  ${GREEN}make train${NC}"
echo ""
echo -e "  ${YELLOW}# 3. Promova para Production no MLflow UI:${NC}"
echo -e "  ${GREEN}http://localhost:5000${NC}"
echo ""
echo -e "  ${YELLOW}# 4. Suba a API:${NC}"
echo -e "  cd ${API_DIR}"
echo -e "  ${GREEN}make up${NC}    # Docker Compose (API + MLflow)"
echo -e "  ${GREEN}make dev${NC}   # ou servidor local"
echo ""
echo -e "  ${YELLOW}# 5. Teste a API:${NC}"
echo -e "  ${GREEN}curl -X POST http://localhost:8000/api/v1/predict \\${NC}"
echo -e "  ${GREEN}     -H 'Content-Type: application/json' \\${NC}"
echo -e "  ${GREEN}     -d '{\"features\": [1.2, 3.4, 5.6]}'${NC}"
echo ""

```

O script automatiza a criaÃ§Ã£o de um projeto MLOps completo com dois repositÃ³rios independentes, cada um com seu prÃ³prio ambiente UV, dependÃªncias e estrutura de pastas.

---

## O que o script cria

`text[destino]/
â”œâ”€â”€ fraud-detector-ds/    â† experimentaÃ§Ã£o, treino, dados
â””â”€â”€ fraud-detector-api/   â† serving em produÃ§Ã£o com FastAPI`

---

## Repo DS (`-ds`)

Focado no ciclo de **experimentaÃ§Ã£o e treino**. Estrutura principal:

- **`src/<pkg>/`** â€” pacote Python com mÃ³dulos separados por responsabilidade: `data.py`, `features.py`, `train.py`, `evaluate.py`, `visualize.py`, `utils.py`
- **`data/`** â€” 4 camadas (`raw`, `processed`, `interim`, `external`), todas versionadas pelo **DVC**
- **`notebooks/`** â€” dividido em `exploratory`, `modeling` e `reports`
- **`models/`** â€” artefatos treinados (`.pkl`, `.pt`), nÃ£o commitados no Git
- **`configs/config.yaml`** â€” hiperparÃ¢metros, caminhos e configuraÃ§Ãµes do MLflow
- **DependÃªncias instaladas:** `pandas`, `numpy`, `scikit-learn`, `matplotlib`, `seaborn`, `plotly`, `mlflow`, `dvc`, `loguru`, `joblib`
- **Dev:** `jupyterlab`, `ruff`, `pytest`, `ipykernel`

---

## Repo API (`-api`)

Focado no **serving em produÃ§Ã£o**. Estrutura principal:

- **`app/`** â€” pacote FastAPI organizado em `routers/`, `schemas/`, `services/`, `middleware/`
- **`app/services/model.py`** â€” `ModelService` com lÃ³gica dupla: carrega do **MLflow Model Registry** em produÃ§Ã£o ou de um `.pkl` local em desenvolvimento
- **`app/schemas/predict.py`** â€” `PredictRequest` e `PredictResponse` com validaÃ§Ã£o Pydantic
- **`Dockerfile`** â€” imagem slim com UV para builds eficientes
- **`docker-compose.yml`** â€” orquestra API + MLflow juntos
- **DependÃªncias instaladas:** `fastapi`, `uvicorn`, `pydantic`, `mlflow`, `joblib`, `pandas`, `numpy`
- **Dev:** `pytest`, `httpx`, `ruff`

---

## Tecnologias e decisÃµes de design

| Camada | Ferramenta | Papel |
| --- | --- | --- |
| Gerenciador de pacotes | **UV** | Ambientes virtuais e dependÃªncias em ambos os repos |
| Build backend | **Hatchling** | Empacotamento com `src/` layout |
| Versionamento de dados | **DVC** | Rastreia arquivos pesados fora do Git |
| Experiment tracking | **MLflow** | Loga mÃ©tricas, parÃ¢metros e registra modelos |
| Serving | **FastAPI** | API assÃ­ncrona com validaÃ§Ã£o automÃ¡tica |
| ContainerizaÃ§Ã£o | **Docker Compose** | Sobe API + MLflow com um comando |
| Qualidade de cÃ³digo | **Ruff** | Linting e formataÃ§Ã£o |

---

## Fluxo completo

`textdados (DVC) â†’ treino (MLflow Experiment)
                    â†’ Model Registry "Production"
                            â†’ API carrega automaticamente
                                    â†’ POST /api/v1/predict`

O **artefato de modelo** Ã© o Ãºnico ponto de contato entre os dois repos â€” nunca viaja via Git, sempre pelo MLflow Registry ou via DVC.
